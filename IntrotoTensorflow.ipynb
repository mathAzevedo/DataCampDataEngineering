{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IntrotoTensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO+/80QADWjMBKOgYpja2GK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mossytreesandferns/DataCampDataEngineering/blob/master/IntrotoTensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WZ2G47xaXrY",
        "colab_type": "code",
        "outputId": "d1b42296-4e44-41ac-e52a-4febd55b536e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#Mount my drive- run the code, go to the link, accept.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5EGpsCIasLh",
        "colab_type": "code",
        "outputId": "9fdb1b24-40a1-4d9e-aa09-8dcf61d48ee5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Change working directory to make it easier to access the files\n",
        "import os\n",
        "os.chdir(\"/content/gdrive/My Drive/Tensorflow/\")\n",
        "os.getcwd()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/My Drive/Tensorflow'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix-GwTf8asYV",
        "colab_type": "text"
      },
      "source": [
        "# Intro to Tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0y2UXT8asby",
        "colab_type": "text"
      },
      "source": [
        "### Constants and Variables\n",
        "constants - can't be changed <br>\n",
        "tensors - variables <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGWOq_eRas11",
        "colab_type": "code",
        "outputId": "aa960dd0-77b0-4c2b-95b3-bccb1180c239",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# 0d tensor\n",
        "d0 = tf.ones(1,)\n",
        "\n",
        "# 1d tensor\n",
        "d1 = tf.ones(2,)\n",
        "\n",
        "# 2d tensor\n",
        "d2 = tf.ones(2,3)\n",
        "\n",
        "# 3d tensor\n",
        "d3 = tf.ones(2,3,2)\n",
        "d2, d3"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 1], dtype=int32)>,\n",
              " <tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 1], dtype=int32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tD9SRuZAe1fm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import constant\n",
        "\n",
        "# Constants are matrices made up of one number\n",
        "\n",
        "a = constant(4, shape=[2,3])\n",
        "b = constant(7, shape=[3,3,3])\n",
        "c = constant([1,2,3,4], shape=[2,2]) # What's the unit in each spot in the matrix? why doesn't [2,2,2] work?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUpSLCz1gj3E",
        "colab_type": "text"
      },
      "source": [
        "tf.zeros_like(input_tensor) - creates tensor of zeros shaped like input tensor <br>\n",
        "tf.ones_like(input_tensor) - creates tensor of ones shaped like input tensor<br>\n",
        "tf.fill()<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAXf3CDPfqqM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating Variables formally\n",
        "\n",
        "a = tf.Variable([1,2,3,4,5,6],dtype=tf.float32)\n",
        "b = tf.Variable([1,2,3,4,5,6],dtype=tf.int16)\n",
        "\n",
        "# Creating constant\n",
        "\n",
        "c = tf.constant(2, tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX100xjQiB9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d1 = tf.multiply(a,c)\n",
        "d2 = a*c # also okay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNvzkU99iYEG",
        "colab_type": "code",
        "outputId": "ff10ebab-fb86-45d3-ad11-6839f40d2c5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        }
      },
      "source": [
        "# not okay\n",
        "d3 = b*c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-f833c47ce3c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# not okay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0md3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_run_op\u001b[0;34m(a, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_oper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_run_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_oper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    982\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 984\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    985\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_mul_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1281\u001b[0m   \u001b[0mis_tensor_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mis_tensor_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1283\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Case: Dense * Sparse.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   6087\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6088\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6089\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6090\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6091\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6651\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6652\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6653\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6654\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: cannot compute Mul as input #1(zero-based) was expected to be a int16 tensor but is a float tensor [Op:Mul]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bjDRNWL281u",
        "colab_type": "text"
      },
      "source": [
        "Note that tensorflow version 2.0 allows you to use data as either a numpy array or a tensorflow constant object. Using a constant will ensure that any operations performed with that object are done in tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H90ZqH5risf8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import constant from TensorFlow\n",
        "from tensorflow import constant\n",
        "\n",
        "# Convert the credit_numpy array into a tensorflow constant\n",
        "credit_constant = constant(credit_numpy) # credit_numpy is a 2d array-these can be pssed in as constants\n",
        "\n",
        "# Print constant datatype\n",
        "print('The datatype is:', credit_constant.dtype)\n",
        "\n",
        "# Print constant shape\n",
        "print('The shape is:', credit_constant.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEghrmbx4UB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the 1-dimensional variable A1\n",
        "A1 = Variable([1, 2, 3, 4])\n",
        "\n",
        "# Print the variable A1\n",
        "print(A1)\n",
        "\n",
        "# Convert A1 to a numpy array and assign it to B1\n",
        "B1 = A1.numpy()\n",
        "\n",
        "# Print B1\n",
        "print(B1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8i-RkqjO4Ysm",
        "colab_type": "text"
      },
      "source": [
        "### Basic Operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcljTUSg4arL",
        "colab_type": "code",
        "outputId": "a577cd40-92c3-4f8e-ac2a-67f6b84f841e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from tensorflow import constant, add\n",
        "\n",
        "# tensors must have the same shape for addition \n",
        "\n",
        "# Define 0d tensors\n",
        "t1 = tf.constant([4])\n",
        "t2 = tf.constant([5])\n",
        "\n",
        "# Define 1d tensors\n",
        "t3 = tf.constant([1,2])\n",
        "t4 = tf.constant([3,4])\n",
        "\n",
        "# Define 2d tensors\n",
        "t5 = tf.constant([[1,2],[3,4]])\n",
        "t6 = tf.constant([[5,6],[7,8]])\n",
        "\n",
        "A0 = add(t1, t2)\n",
        "A1 = add(t3, t4)\n",
        "A2 = add(t5, t6)\n",
        "\n",
        "print(A0,A1,A2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([9], shape=(1,), dtype=int32) tf.Tensor([4 6], shape=(2,), dtype=int32) tf.Tensor(\n",
            "[[ 6  8]\n",
            " [10 12]], shape=(2, 2), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "up6k_MIfJb57",
        "colab_type": "code",
        "outputId": "c8709b2c-90b3-4770-c924-0de441dbf5cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# to multiply matrix of m x n, the next matrix has to be n x anything\n",
        "# multiply() matrices have to be the same shape, matmul() matrices have to be mXn and nXanything\n",
        "\n",
        "from tensorflow import constant, ones, matmul, multiply\n",
        "A3 = multiply(t5,t6)\n",
        "print(A3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[ 5 12]\n",
            " [21 32]], shape=(2, 2), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTA7E2mtMcD5",
        "colab_type": "code",
        "outputId": "b6a253bf-6290-452b-eb5d-022297866ecc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "from tensorflow import ones, reduce_sum\n",
        "\n",
        "t = ones([2,3,4])\n",
        "\n",
        "x = reduce_sum(t)\n",
        "y = reduce_sum(t,0)\n",
        "z = reduce_sum(t,1)\n",
        "w = reduce_sum(t,2)\n",
        "print(x,y,z,w)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(24.0, shape=(), dtype=float32) tf.Tensor(\n",
            "[[2. 2. 2. 2.]\n",
            " [2. 2. 2. 2.]\n",
            " [2. 2. 2. 2.]], shape=(3, 4), dtype=float32) tf.Tensor(\n",
            "[[3. 3. 3. 3.]\n",
            " [3. 3. 3. 3.]], shape=(2, 4), dtype=float32) tf.Tensor(\n",
            "[[4. 4. 4.]\n",
            " [4. 4. 4.]], shape=(2, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SznEwBkjTNfM",
        "colab_type": "code",
        "outputId": "0f116b0d-215c-48e5-8e4b-5ce596216f14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "reduce_sum(t,0).numpy() # adding numpy() prints out array() without extraneous info (?)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2., 2., 2., 2.],\n",
              "       [2., 2., 2., 2.],\n",
              "       [2., 2., 2., 2.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o_O_H7xOmr9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define tensors A1 and A23 as constants\n",
        "A1 = constant([1, 2, 3, 4])\n",
        "A23 = constant([[1, 2, 3], [1, 6, 4]])\n",
        "\n",
        "# Define B1 and B23 to have the correct shape\n",
        "B1 = ones_like(A1)\n",
        "B23 = ones_like(A23)\n",
        "\n",
        "# Perform element-wise multiplication\n",
        "C1 = multiply(A1, B1)\n",
        "C23 = multiply(A23,B23)\n",
        "\n",
        "# Print the tensors C1 and C23\n",
        "print('C1: {}'.format(C1.numpy()))\n",
        "print('C23: {}'.format(C23.numpy()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_K36N3JPyqS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define features, params, and bill as constants\n",
        "features = constant([[2, 24], [2, 26], [2, 57], [1, 37]])\n",
        "params = constant([[1000], [150]])\n",
        "bill = constant([[3913], [2682], [8617], [64400]])\n",
        "\n",
        "# Compute billpred using features and params\n",
        "billpred = matmul(features,params)\n",
        "\n",
        "# Compute and print the error\n",
        "error = bill - billpred\n",
        "print(error.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWdmkFj0Ty6k",
        "colab_type": "text"
      },
      "source": [
        "### Advanced Operations\n",
        "\n",
        "gradient(), reshape(), and random()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRZkXewCT132",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gradient()- computes slope of function at a point, finds min or max of a function (where slope is 0) to minimize or maximize a problem\n",
        "# reshape()- reshapes a tensor\n",
        "# random()- populates a tensor with entries from a probability distribution"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svCY42-MZSRx",
        "colab_type": "code",
        "outputId": "3d1747e5-9772-4f48-c7d8-6956b6036982",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "# GradientTape() performs automatic differentiation\n",
        "\n",
        "# Define x\n",
        "x = tf.Variable(-1.0)\n",
        "\n",
        "# Define y in an instance of GradientTape()\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(x)\n",
        "  y = tf.multiply(x,x)  # This translates to y=x^2\n",
        "\n",
        "# Evaluate gradient of y=x^2 at x=-1\n",
        "g = tape.gradient(y,x)\n",
        "print(g.numpy())  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqoYdYfNb98S",
        "colab_type": "text"
      },
      "source": [
        "Images as tensors. Grayscale images of values between 0 and 255 (2d matrix of image pixels).  Image gets reshaped to m X 1 vector... (m X n X n X 1 --> m X 1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQSg4ANzaSVx",
        "colab_type": "code",
        "outputId": "e0dffb37-1cb6-46cc-8718-f2c3ed4846f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Generate grayscale image\n",
        "im = tf.random.uniform([2,2],maxval=255,dtype='int32')\n",
        "# Reshape grayscale image\n",
        "im = tf.reshape(im,[2*2, 1]) # multiply all dimensions of original tensor together to get number of rows and make ti one column\n",
        "im"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4, 1), dtype=int32, numpy=\n",
              "array([[ 33],\n",
              "       [ 90],\n",
              "       [ 76],\n",
              "       [213]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9bcTasBfOSd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate color image\n",
        "color = tf.random.uniform([2,2,3], maxval=255, dtype='int32')\n",
        "# Reshape color image\n",
        "color = tf.reshape(color, [2*2,3]) # anomalous"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_Dmia5mf6ao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reshape the grayscale image tensor into a vector\n",
        "gray_vector = reshape(gray_tensor, (784, 1)) # multiply all dimensions of original tensor together to get number of rows and make ti one column\n",
        "\n",
        "# Reshape the color image tensor into a vector\n",
        "color_vector = reshape(color_tensor, (2352, 1))# multiply all dimensions of original tensor together to get number of rows and make ti one column"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioAByV-ahSEk",
        "colab_type": "code",
        "outputId": "59a6827a-e0f2-42c4-a300-4dc3120eb6cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import GradientTape\n",
        "\n",
        "def compute_gradient(x0):\n",
        "  \t# Define x as a variable with an initial value of x0\n",
        "\tx = tf.Variable(x0)\n",
        "\twith GradientTape() as tape:\n",
        "\t\ttape.watch(x)\n",
        "        # Define y using the multiply operation\n",
        "\t\ty = multiply(x,x)\n",
        "    # Return the gradient of y with respect to x\n",
        "\treturn tape.gradient(y, x).numpy()\n",
        "\n",
        "# Compute and print gradients at x = -1, 1, and 0\n",
        "print(compute_gradient(-1.0))\n",
        "print(compute_gradient(1.0))\n",
        "print(compute_gradient(0.0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-2.0\n",
            "2.0\n",
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_jeYyB-hTBd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reshape model from a 1x3 to a 3x1 tensor\n",
        "model = reshape(model, (1*3, 1))\n",
        "\n",
        "# Multiply letter by model\n",
        "output = matmul(letter, model)\n",
        "\n",
        "# Sum over output and print prediction using the numpy method\n",
        "prediction = reduce_sum(output)\n",
        "print(prediction.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kKtPsiaixvc",
        "colab_type": "text"
      },
      "source": [
        "# Linear Models\n",
        "### Input Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8NUJlnri-1s",
        "colab_type": "code",
        "outputId": "283c9c5a-cfb8-463b-b510-0501c47a8e25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "data = pd.read_csv('kc_house_data.csv')\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>price</th>\n",
              "      <th>bedrooms</th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>sqft_living</th>\n",
              "      <th>sqft_lot</th>\n",
              "      <th>floors</th>\n",
              "      <th>waterfront</th>\n",
              "      <th>view</th>\n",
              "      <th>condition</th>\n",
              "      <th>grade</th>\n",
              "      <th>sqft_above</th>\n",
              "      <th>sqft_basement</th>\n",
              "      <th>yr_built</th>\n",
              "      <th>yr_renovated</th>\n",
              "      <th>zipcode</th>\n",
              "      <th>lat</th>\n",
              "      <th>long</th>\n",
              "      <th>sqft_living15</th>\n",
              "      <th>sqft_lot15</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7129300520</td>\n",
              "      <td>20141013T000000</td>\n",
              "      <td>221900.0</td>\n",
              "      <td>3</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1180</td>\n",
              "      <td>5650</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1180</td>\n",
              "      <td>0</td>\n",
              "      <td>1955</td>\n",
              "      <td>0</td>\n",
              "      <td>98178</td>\n",
              "      <td>47.5112</td>\n",
              "      <td>-122.257</td>\n",
              "      <td>1340</td>\n",
              "      <td>5650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6414100192</td>\n",
              "      <td>20141209T000000</td>\n",
              "      <td>538000.0</td>\n",
              "      <td>3</td>\n",
              "      <td>2.25</td>\n",
              "      <td>2570</td>\n",
              "      <td>7242</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>2170</td>\n",
              "      <td>400</td>\n",
              "      <td>1951</td>\n",
              "      <td>1991</td>\n",
              "      <td>98125</td>\n",
              "      <td>47.7210</td>\n",
              "      <td>-122.319</td>\n",
              "      <td>1690</td>\n",
              "      <td>7639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5631500400</td>\n",
              "      <td>20150225T000000</td>\n",
              "      <td>180000.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1.00</td>\n",
              "      <td>770</td>\n",
              "      <td>10000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>770</td>\n",
              "      <td>0</td>\n",
              "      <td>1933</td>\n",
              "      <td>0</td>\n",
              "      <td>98028</td>\n",
              "      <td>47.7379</td>\n",
              "      <td>-122.233</td>\n",
              "      <td>2720</td>\n",
              "      <td>8062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2487200875</td>\n",
              "      <td>20141209T000000</td>\n",
              "      <td>604000.0</td>\n",
              "      <td>4</td>\n",
              "      <td>3.00</td>\n",
              "      <td>1960</td>\n",
              "      <td>5000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>1050</td>\n",
              "      <td>910</td>\n",
              "      <td>1965</td>\n",
              "      <td>0</td>\n",
              "      <td>98136</td>\n",
              "      <td>47.5208</td>\n",
              "      <td>-122.393</td>\n",
              "      <td>1360</td>\n",
              "      <td>5000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1954400510</td>\n",
              "      <td>20150218T000000</td>\n",
              "      <td>510000.0</td>\n",
              "      <td>3</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1680</td>\n",
              "      <td>8080</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>1680</td>\n",
              "      <td>0</td>\n",
              "      <td>1987</td>\n",
              "      <td>0</td>\n",
              "      <td>98074</td>\n",
              "      <td>47.6168</td>\n",
              "      <td>-122.045</td>\n",
              "      <td>1800</td>\n",
              "      <td>7503</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           id             date     price  ...     long  sqft_living15  sqft_lot15\n",
              "0  7129300520  20141013T000000  221900.0  ... -122.257           1340        5650\n",
              "1  6414100192  20141209T000000  538000.0  ... -122.319           1690        7639\n",
              "2  5631500400  20150225T000000  180000.0  ... -122.233           2720        8062\n",
              "3  2487200875  20141209T000000  604000.0  ... -122.393           1360        5000\n",
              "4  1954400510  20150218T000000  510000.0  ... -122.045           1800        7503\n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31t66u36sw3x",
        "colab_type": "code",
        "outputId": "57567f5f-87d5-46dc-8166-01d47980c657",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21613, 21)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGDQicV1rEW4",
        "colab_type": "text"
      },
      "source": [
        "Using native numpy method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHa0JuZNn9nm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert to numpy array\n",
        "housing = np.array(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik5a_ZwApXyS",
        "colab_type": "code",
        "outputId": "7adfa118-6a3b-4b58-f94d-fb47ea017349",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Convert price columns\n",
        "price = np.array(housing[:,2], dtype=np.float32) # had to change column name from label to numeric index\n",
        "\n",
        "# Convert the waterfront column\n",
        "waterfront = np.array(housing[:,8], dtype=np.bool) # had to change column name from label to numeric index\n",
        "price, waterfront"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([221900., 538000., 180000., ..., 402101., 400000., 325000.],\n",
              "       dtype=float32), array([False, False, False, ..., False, False, False]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_CUy-4erHjY",
        "colab_type": "text"
      },
      "source": [
        "Using a tensorflow method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9CnQwXPpoYO",
        "colab_type": "code",
        "outputId": "c91a48de-a7e9-4ecf-a462-25ccc0146e0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "housing = pd.read_csv('kc_house_data.csv')\n",
        "price = tf.cast(housing['price'], tf.float32)\n",
        "waterfront = tf.cast(housing['waterfront'], tf.bool)\n",
        "price, waterfront"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(21613,), dtype=float32, numpy=\n",
              " array([221900., 538000., 180000., ..., 402101., 400000., 325000.],\n",
              "       dtype=float32)>,\n",
              " <tf.Tensor: shape=(21613,), dtype=bool, numpy=array([False, False, False, ..., False, False, False])>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PO7BI66teHg",
        "colab_type": "text"
      },
      "source": [
        "### Loss Functions\n",
        "<br>\n",
        "Loss functions explain how well model fits data. Higher value --> worse fit.<br>\n",
        "<br>\n",
        "1. Mean squared error (MSE) - penalizes outlier, strong sensitivity near 0.<br>\n",
        "2. Mean absolute error (MAE) - corresponds linearly with error, low sensitivity near 0. Minimize impact of outliers.<br>\n",
        "3. Huber loss - like MSE near 0 and MEA beyond. Minimize impact of outliers.<br>\n",
        "<br>\n",
        "tf.keras.losses.mse(), tf.keras.losses.mae(), tf.keras.losses.Huber()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnsOwJgjry_Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use two tensors, actual and predicted values\n",
        "loss = tf.keras.losses.mse(actual, predicted) # returns average of squared difference "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQ6zv3Sszf5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define linear regression model\n",
        "from tensorflow import keras\n",
        "\n",
        "def linear_regression(intercept, slope=slope, features=features):\n",
        "  return intercept + features*slope\n",
        "\n",
        "def loss_function(intercept, slope, targets=targets, features=features): # Run twice: once with test data, once with default data\n",
        "  predictions = linear_regression(intercept, slope)\n",
        "\n",
        "  return tf.keras.losses.mse(targets, predictions)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTyf-grh3g6J",
        "colab_type": "text"
      },
      "source": [
        "### Linear Regression\n",
        "<br>\n",
        "In linear regression we plot data points to see the trend in the data to observe possible correlation between input and output variables.  Multivariate regressions are possible.\n",
        "<br>\n",
        "Transforming the data into a log of itself minimizes the impact of variability in data points and skewedness of data and converts it into a set that is closer to a normalized set of data points.\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TeDrKdI6cuQ",
        "colab_type": "code",
        "outputId": "be0381d5-65d8-4edf-e506-e473c04bcc83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "housing = pd.read_csv('kc_house_data.csv')\n",
        "housing = np.array(data)\n",
        "\n",
        "# Define targets and predictions\n",
        "size = np.array(housing[:,5], dtype=np.float32)\n",
        "price = np.array(housing[:,2], dtype=np.float32) \n",
        "\n",
        "# Define intercept and slope\n",
        "intercept = tf.Variable(0.1, dtype=np.float32)\n",
        "slope = tf.Variable(0.1, dtype=np.float32)\n",
        "\n",
        "size, price"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1180., 2570.,  770., ..., 1020., 1600., 1020.], dtype=float32),\n",
              " array([221900., 538000., 180000., ..., 402101., 400000., 325000.],\n",
              "       dtype=float32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7T9gKIvn7Wyv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a linear regression model\n",
        "def linear_regression(intercept, slope, features=size):\n",
        "  return intercept + slope*features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtpXDaHH-Qdr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define loss function\n",
        "def loss_function(intercept, slope, targets=price, features=size):\n",
        "  predictions = linear_regression(intercept, slope)\n",
        "  return tf.keras.losses.mse(targets, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWEr21jV-7OY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define and optimization method\n",
        "opt = tf.keras.optimizers.Adam()  # Optimization methods change the slope and intercept in directions that decreases the value of losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B97aXCBW_buI",
        "colab_type": "code",
        "outputId": "80da57a1-ee4c-4dd9-c024-2033a94fce40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Utilizeing opt\n",
        "\n",
        "for i in range(1000):\n",
        "  opt.minimize(lambda: loss_function(intercept, slope), var_list=[intercept, slope])\n",
        "  print(loss_function(intercept, slope).numpy()) # Values are supposed to fall from 10 to < 1... why is mine different?"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "420785060000.0\n",
            "420782400000.0\n",
            "420779720000.0\n",
            "420776970000.0\n",
            "420774250000.0\n",
            "420771530000.0\n",
            "420768780000.0\n",
            "420766100000.0\n",
            "420763470000.0\n",
            "420760850000.0\n",
            "420758100000.0\n",
            "420755370000.0\n",
            "420752620000.0\n",
            "420749900000.0\n",
            "420747300000.0\n",
            "420744600000.0\n",
            "420741900000.0\n",
            "420739200000.0\n",
            "420736500000.0\n",
            "420733800000.0\n",
            "420731060000.0\n",
            "420728400000.0\n",
            "420725750000.0\n",
            "420722870000.0\n",
            "420720280000.0\n",
            "420717630000.0\n",
            "420714870000.0\n",
            "420712200000.0\n",
            "420709470000.0\n",
            "420706880000.0\n",
            "420704100000.0\n",
            "420701370000.0\n",
            "420698720000.0\n",
            "420695970000.0\n",
            "420693300000.0\n",
            "420690600000.0\n",
            "420687900000.0\n",
            "420685120000.0\n",
            "420682500000.0\n",
            "420679800000.0\n",
            "420677120000.0\n",
            "420674400000.0\n",
            "420671650000.0\n",
            "420669030000.0\n",
            "420666280000.0\n",
            "420663660000.0\n",
            "420660870000.0\n",
            "420658200000.0\n",
            "420655500000.0\n",
            "420652840000.0\n",
            "420650160000.0\n",
            "420647470000.0\n",
            "420644700000.0\n",
            "420642000000.0\n",
            "420639300000.0\n",
            "420636620000.0\n",
            "420633940000.0\n",
            "420631200000.0\n",
            "420628530000.0\n",
            "420625800000.0\n",
            "420623120000.0\n",
            "420620440000.0\n",
            "420617750000.0\n",
            "420615060000.0\n",
            "420612340000.0\n",
            "420609620000.0\n",
            "420606940000.0\n",
            "420604250000.0\n",
            "420601560000.0\n",
            "420598840000.0\n",
            "420596100000.0\n",
            "420593400000.0\n",
            "420590700000.0\n",
            "420588060000.0\n",
            "420585370000.0\n",
            "420582600000.0\n",
            "420579900000.0\n",
            "420577280000.0\n",
            "420574630000.0\n",
            "420571900000.0\n",
            "420569150000.0\n",
            "420566430000.0\n",
            "420563750000.0\n",
            "420560960000.0\n",
            "420558340000.0\n",
            "420555620000.0\n",
            "420552930000.0\n",
            "420550280000.0\n",
            "420547600000.0\n",
            "420544940000.0\n",
            "420542200000.0\n",
            "420539470000.0\n",
            "420536800000.0\n",
            "420534100000.0\n",
            "420531370000.0\n",
            "420528700000.0\n",
            "420526000000.0\n",
            "420523280000.0\n",
            "420520600000.0\n",
            "420517900000.0\n",
            "420515150000.0\n",
            "420512500000.0\n",
            "420509800000.0\n",
            "420507120000.0\n",
            "420504470000.0\n",
            "420501750000.0\n",
            "420499030000.0\n",
            "420496340000.0\n",
            "420493560000.0\n",
            "420490800000.0\n",
            "420488200000.0\n",
            "420485500000.0\n",
            "420482800000.0\n",
            "420480100000.0\n",
            "420477470000.0\n",
            "420474700000.0\n",
            "420472000000.0\n",
            "420469370000.0\n",
            "420466620000.0\n",
            "420463940000.0\n",
            "420461250000.0\n",
            "420458530000.0\n",
            "420455800000.0\n",
            "420453120000.0\n",
            "420450440000.0\n",
            "420447750000.0\n",
            "420445060000.0\n",
            "420442340000.0\n",
            "420439700000.0\n",
            "420436970000.0\n",
            "420434250000.0\n",
            "420431560000.0\n",
            "420428870000.0\n",
            "420426120000.0\n",
            "420423470000.0\n",
            "420420700000.0\n",
            "420418030000.0\n",
            "420415340000.0\n",
            "420412700000.0\n",
            "420410000000.0\n",
            "420407250000.0\n",
            "420404560000.0\n",
            "420401900000.0\n",
            "420399150000.0\n",
            "420396470000.0\n",
            "420393780000.0\n",
            "420391120000.0\n",
            "420388440000.0\n",
            "420385700000.0\n",
            "420383000000.0\n",
            "420380280000.0\n",
            "420377600000.0\n",
            "420374870000.0\n",
            "420372200000.0\n",
            "420369500000.0\n",
            "420366880000.0\n",
            "420364120000.0\n",
            "420361470000.0\n",
            "420358720000.0\n",
            "420356030000.0\n",
            "420353340000.0\n",
            "420350700000.0\n",
            "420348000000.0\n",
            "420345300000.0\n",
            "420342530000.0\n",
            "420339780000.0\n",
            "420337120000.0\n",
            "420334530000.0\n",
            "420331780000.0\n",
            "420329130000.0\n",
            "420326340000.0\n",
            "420323660000.0\n",
            "420320970000.0\n",
            "420318280000.0\n",
            "420315600000.0\n",
            "420312900000.0\n",
            "420310300000.0\n",
            "420307500000.0\n",
            "420304800000.0\n",
            "420302130000.0\n",
            "420299400000.0\n",
            "420296720000.0\n",
            "420294000000.0\n",
            "420291250000.0\n",
            "420288560000.0\n",
            "420285870000.0\n",
            "420283220000.0\n",
            "420280530000.0\n",
            "420277850000.0\n",
            "420275100000.0\n",
            "420272470000.0\n",
            "420269700000.0\n",
            "420267070000.0\n",
            "420264380000.0\n",
            "420261660000.0\n",
            "420258900000.0\n",
            "420256220000.0\n",
            "420253530000.0\n",
            "420250880000.0\n",
            "420248200000.0\n",
            "420245540000.0\n",
            "420242750000.0\n",
            "420240030000.0\n",
            "420237340000.0\n",
            "420234720000.0\n",
            "420231940000.0\n",
            "420229250000.0\n",
            "420226630000.0\n",
            "420223880000.0\n",
            "420221200000.0\n",
            "420218540000.0\n",
            "420215780000.0\n",
            "420213060000.0\n",
            "420210380000.0\n",
            "420207760000.0\n",
            "420205000000.0\n",
            "420202300000.0\n",
            "420199600000.0\n",
            "420196970000.0\n",
            "420194200000.0\n",
            "420191500000.0\n",
            "420188800000.0\n",
            "420186200000.0\n",
            "420183500000.0\n",
            "420180750000.0\n",
            "420178070000.0\n",
            "420175350000.0\n",
            "420172660000.0\n",
            "420169970000.0\n",
            "420167250000.0\n",
            "420164530000.0\n",
            "420161780000.0\n",
            "420159100000.0\n",
            "420156470000.0\n",
            "420153850000.0\n",
            "420151100000.0\n",
            "420148400000.0\n",
            "420145800000.0\n",
            "420143070000.0\n",
            "420140320000.0\n",
            "420137630000.0\n",
            "420134900000.0\n",
            "420132200000.0\n",
            "420129500000.0\n",
            "420126820000.0\n",
            "420124160000.0\n",
            "420121440000.0\n",
            "420118720000.0\n",
            "420116070000.0\n",
            "420113380000.0\n",
            "420110630000.0\n",
            "420107940000.0\n",
            "420105260000.0\n",
            "420102500000.0\n",
            "420099880000.0\n",
            "420097200000.0\n",
            "420094500000.0\n",
            "420091800000.0\n",
            "420089070000.0\n",
            "420086400000.0\n",
            "420083700000.0\n",
            "420080940000.0\n",
            "420078320000.0\n",
            "420075600000.0\n",
            "420072850000.0\n",
            "420070260000.0\n",
            "420067540000.0\n",
            "420064820000.0\n",
            "420062130000.0\n",
            "420059450000.0\n",
            "420056730000.0\n",
            "420054070000.0\n",
            "420051300000.0\n",
            "420048600000.0\n",
            "420045950000.0\n",
            "420043260000.0\n",
            "420040570000.0\n",
            "420037920000.0\n",
            "420035260000.0\n",
            "420032500000.0\n",
            "420029830000.0\n",
            "420027100000.0\n",
            "420024500000.0\n",
            "420021700000.0\n",
            "420018950000.0\n",
            "420016320000.0\n",
            "420013600000.0\n",
            "420010850000.0\n",
            "420008260000.0\n",
            "420005600000.0\n",
            "420002860000.0\n",
            "420000170000.0\n",
            "419997450000.0\n",
            "419994830000.0\n",
            "419992140000.0\n",
            "419989400000.0\n",
            "419986700000.0\n",
            "419983920000.0\n",
            "419981260000.0\n",
            "419978580000.0\n",
            "419975900000.0\n",
            "419973200000.0\n",
            "419970500000.0\n",
            "419967830000.0\n",
            "419965140000.0\n",
            "419962420000.0\n",
            "419959700000.0\n",
            "419957000000.0\n",
            "419954330000.0\n",
            "419951640000.0\n",
            "419948950000.0\n",
            "419946270000.0\n",
            "419943550000.0\n",
            "419940930000.0\n",
            "419938140000.0\n",
            "419935550000.0\n",
            "419932770000.0\n",
            "419930080000.0\n",
            "419927300000.0\n",
            "419924670000.0\n",
            "419922020000.0\n",
            "419919360000.0\n",
            "419916600000.0\n",
            "419914000000.0\n",
            "419911200000.0\n",
            "419908520000.0\n",
            "419905830000.0\n",
            "419903140000.0\n",
            "419900360000.0\n",
            "419897740000.0\n",
            "419895020000.0\n",
            "419892330000.0\n",
            "419889640000.0\n",
            "419886960000.0\n",
            "419884240000.0\n",
            "419881550000.0\n",
            "419878900000.0\n",
            "419876240000.0\n",
            "419873550000.0\n",
            "419870800000.0\n",
            "419868180000.0\n",
            "419865460000.0\n",
            "419862800000.0\n",
            "419860020000.0\n",
            "419857330000.0\n",
            "419854680000.0\n",
            "419851930000.0\n",
            "419849200000.0\n",
            "419846550000.0\n",
            "419843900000.0\n",
            "419841150000.0\n",
            "419838460000.0\n",
            "419835770000.0\n",
            "419833120000.0\n",
            "419830430000.0\n",
            "419827680000.0\n",
            "419824960000.0\n",
            "419822340000.0\n",
            "419819650000.0\n",
            "419816960000.0\n",
            "419814200000.0\n",
            "419811500000.0\n",
            "419808870000.0\n",
            "419806180000.0\n",
            "419803400000.0\n",
            "419800800000.0\n",
            "419798120000.0\n",
            "419795340000.0\n",
            "419792650000.0\n",
            "419790000000.0\n",
            "419787300000.0\n",
            "419784620000.0\n",
            "419781940000.0\n",
            "419779150000.0\n",
            "419776460000.0\n",
            "419773740000.0\n",
            "419771060000.0\n",
            "419768370000.0\n",
            "419765750000.0\n",
            "419763060000.0\n",
            "419760300000.0\n",
            "419757700000.0\n",
            "419754970000.0\n",
            "419752280000.0\n",
            "419749530000.0\n",
            "419746870000.0\n",
            "419744120000.0\n",
            "419741500000.0\n",
            "419738700000.0\n",
            "419736130000.0\n",
            "419733440000.0\n",
            "419730720000.0\n",
            "419727970000.0\n",
            "419725300000.0\n",
            "419722630000.0\n",
            "419719940000.0\n",
            "419717250000.0\n",
            "419714560000.0\n",
            "419711840000.0\n",
            "419709100000.0\n",
            "419706470000.0\n",
            "419703750000.0\n",
            "419701100000.0\n",
            "419698400000.0\n",
            "419695700000.0\n",
            "419693000000.0\n",
            "419690300000.0\n",
            "419687700000.0\n",
            "419685000000.0\n",
            "419682300000.0\n",
            "419679600000.0\n",
            "419676850000.0\n",
            "419674130000.0\n",
            "419671500000.0\n",
            "419668750000.0\n",
            "419666130000.0\n",
            "419663380000.0\n",
            "419660730000.0\n",
            "419657940000.0\n",
            "419655300000.0\n",
            "419652570000.0\n",
            "419649880000.0\n",
            "419647200000.0\n",
            "419644400000.0\n",
            "419641800000.0\n",
            "419639100000.0\n",
            "419636400000.0\n",
            "419633800000.0\n",
            "419631040000.0\n",
            "419628420000.0\n",
            "419625700000.0\n",
            "419623000000.0\n",
            "419620320000.0\n",
            "419617540000.0\n",
            "419614920000.0\n",
            "419612160000.0\n",
            "419609540000.0\n",
            "419606820000.0\n",
            "419604140000.0\n",
            "419601450000.0\n",
            "419598800000.0\n",
            "419596080000.0\n",
            "419593360000.0\n",
            "419590670000.0\n",
            "419587980000.0\n",
            "419585300000.0\n",
            "419582600000.0\n",
            "419579820000.0\n",
            "419577230000.0\n",
            "419574480000.0\n",
            "419571830000.0\n",
            "419569140000.0\n",
            "419566420000.0\n",
            "419563800000.0\n",
            "419561050000.0\n",
            "419558360000.0\n",
            "419555670000.0\n",
            "419553020000.0\n",
            "419550270000.0\n",
            "419547580000.0\n",
            "419544900000.0\n",
            "419542270000.0\n",
            "419539550000.0\n",
            "419536800000.0\n",
            "419534100000.0\n",
            "419531460000.0\n",
            "419528770000.0\n",
            "419526080000.0\n",
            "419523330000.0\n",
            "419520600000.0\n",
            "419517920000.0\n",
            "419515240000.0\n",
            "419512550000.0\n",
            "419509900000.0\n",
            "419507200000.0\n",
            "419504520000.0\n",
            "419501830000.0\n",
            "419499150000.0\n",
            "419496460000.0\n",
            "419493770000.0\n",
            "419491050000.0\n",
            "419488330000.0\n",
            "419485650000.0\n",
            "419482960000.0\n",
            "419480270000.0\n",
            "419477650000.0\n",
            "419474900000.0\n",
            "419472180000.0\n",
            "419469430000.0\n",
            "419466770000.0\n",
            "419464200000.0\n",
            "419461430000.0\n",
            "419458700000.0\n",
            "419456120000.0\n",
            "419453340000.0\n",
            "419450650000.0\n",
            "419448000000.0\n",
            "419445300000.0\n",
            "419442620000.0\n",
            "419439940000.0\n",
            "419437200000.0\n",
            "419434560000.0\n",
            "419431840000.0\n",
            "419429100000.0\n",
            "419426440000.0\n",
            "419423680000.0\n",
            "419421060000.0\n",
            "419418370000.0\n",
            "419415700000.0\n",
            "419412970000.0\n",
            "419410200000.0\n",
            "419407530000.0\n",
            "419404870000.0\n",
            "419402200000.0\n",
            "419399500000.0\n",
            "419396800000.0\n",
            "419394130000.0\n",
            "419391500000.0\n",
            "419388750000.0\n",
            "419386060000.0\n",
            "419383300000.0\n",
            "419380630000.0\n",
            "419377940000.0\n",
            "419375250000.0\n",
            "419372630000.0\n",
            "419369940000.0\n",
            "419367260000.0\n",
            "419364570000.0\n",
            "419361850000.0\n",
            "419359160000.0\n",
            "419356380000.0\n",
            "419353800000.0\n",
            "419351100000.0\n",
            "419348400000.0\n",
            "419345630000.0\n",
            "419343000000.0\n",
            "419340300000.0\n",
            "419337600000.0\n",
            "419334900000.0\n",
            "419332230000.0\n",
            "419329500000.0\n",
            "419326820000.0\n",
            "419324130000.0\n",
            "419321480000.0\n",
            "419318820000.0\n",
            "419316070000.0\n",
            "419313350000.0\n",
            "419310700000.0\n",
            "419307980000.0\n",
            "419305300000.0\n",
            "419302670000.0\n",
            "419299920000.0\n",
            "419297260000.0\n",
            "419294580000.0\n",
            "419291820000.0\n",
            "419289200000.0\n",
            "419286420000.0\n",
            "419283730000.0\n",
            "419281100000.0\n",
            "419278360000.0\n",
            "419275700000.0\n",
            "419273050000.0\n",
            "419270230000.0\n",
            "419267640000.0\n",
            "419264950000.0\n",
            "419262270000.0\n",
            "419259550000.0\n",
            "419256800000.0\n",
            "419254140000.0\n",
            "419251450000.0\n",
            "419248770000.0\n",
            "419246080000.0\n",
            "419243460000.0\n",
            "419240700000.0\n",
            "419238020000.0\n",
            "419235300000.0\n",
            "419232680000.0\n",
            "419230000000.0\n",
            "419227300000.0\n",
            "419224520000.0\n",
            "419221930000.0\n",
            "419219240000.0\n",
            "419216520000.0\n",
            "419213840000.0\n",
            "419211120000.0\n",
            "419208430000.0\n",
            "419205740000.0\n",
            "419203060000.0\n",
            "419200430000.0\n",
            "419197680000.0\n",
            "419195000000.0\n",
            "419192340000.0\n",
            "419189650000.0\n",
            "419186970000.0\n",
            "419184280000.0\n",
            "419181560000.0\n",
            "419178870000.0\n",
            "419176200000.0\n",
            "419173430000.0\n",
            "419170780000.0\n",
            "419168030000.0\n",
            "419165340000.0\n",
            "419162620000.0\n",
            "419159930000.0\n",
            "419157250000.0\n",
            "419154620000.0\n",
            "419151970000.0\n",
            "419149300000.0\n",
            "419146560000.0\n",
            "419143880000.0\n",
            "419141160000.0\n",
            "419138470000.0\n",
            "419135780000.0\n",
            "419133100000.0\n",
            "419130400000.0\n",
            "419127700000.0\n",
            "419125000000.0\n",
            "419122280000.0\n",
            "419119630000.0\n",
            "419116900000.0\n",
            "419114220000.0\n",
            "419111530000.0\n",
            "419108850000.0\n",
            "419106200000.0\n",
            "419103500000.0\n",
            "419100820000.0\n",
            "419098170000.0\n",
            "419095450000.0\n",
            "419092820000.0\n",
            "419090070000.0\n",
            "419087400000.0\n",
            "419084730000.0\n",
            "419081980000.0\n",
            "419079360000.0\n",
            "419076570000.0\n",
            "419073900000.0\n",
            "419071260000.0\n",
            "419068480000.0\n",
            "419065800000.0\n",
            "419063170000.0\n",
            "419060480000.0\n",
            "419057800000.0\n",
            "419055100000.0\n",
            "419052420000.0\n",
            "419049700000.0\n",
            "419046950000.0\n",
            "419044330000.0\n",
            "419041640000.0\n",
            "419038950000.0\n",
            "419036330000.0\n",
            "419033650000.0\n",
            "419030930000.0\n",
            "419028170000.0\n",
            "419025500000.0\n",
            "419022700000.0\n",
            "419020080000.0\n",
            "419017400000.0\n",
            "419014770000.0\n",
            "419012000000.0\n",
            "419009300000.0\n",
            "419006680000.0\n",
            "419004020000.0\n",
            "419001300000.0\n",
            "418998600000.0\n",
            "418995900000.0\n",
            "418993200000.0\n",
            "418990520000.0\n",
            "418987900000.0\n",
            "418985150000.0\n",
            "418982460000.0\n",
            "418979800000.0\n",
            "418977050000.0\n",
            "418974370000.0\n",
            "418971750000.0\n",
            "418969060000.0\n",
            "418966370000.0\n",
            "418963700000.0\n",
            "418960900000.0\n",
            "418958280000.0\n",
            "418955600000.0\n",
            "418952870000.0\n",
            "418950200000.0\n",
            "418947560000.0\n",
            "418944800000.0\n",
            "418942120000.0\n",
            "418939440000.0\n",
            "418936720000.0\n",
            "418934030000.0\n",
            "418931300000.0\n",
            "418928660000.0\n",
            "418925940000.0\n",
            "418923300000.0\n",
            "418920560000.0\n",
            "418917880000.0\n",
            "418915220000.0\n",
            "418912530000.0\n",
            "418909780000.0\n",
            "418907160000.0\n",
            "418904470000.0\n",
            "418901800000.0\n",
            "418899100000.0\n",
            "418896450000.0\n",
            "418893760000.0\n",
            "418891070000.0\n",
            "418888400000.0\n",
            "418885700000.0\n",
            "418883000000.0\n",
            "418880230000.0\n",
            "418877540000.0\n",
            "418874900000.0\n",
            "418872230000.0\n",
            "418869500000.0\n",
            "418866820000.0\n",
            "418864100000.0\n",
            "418861420000.0\n",
            "418858730000.0\n",
            "418856040000.0\n",
            "418853360000.0\n",
            "418850730000.0\n",
            "418847980000.0\n",
            "418845300000.0\n",
            "418842640000.0\n",
            "418839900000.0\n",
            "418837200000.0\n",
            "418834580000.0\n",
            "418831900000.0\n",
            "418829170000.0\n",
            "418826500000.0\n",
            "418823900000.0\n",
            "418821200000.0\n",
            "418818500000.0\n",
            "418815740000.0\n",
            "418813050000.0\n",
            "418810400000.0\n",
            "418807740000.0\n",
            "418805020000.0\n",
            "418802340000.0\n",
            "418799650000.0\n",
            "418796930000.0\n",
            "418794270000.0\n",
            "418791560000.0\n",
            "418788870000.0\n",
            "418786150000.0\n",
            "418783460000.0\n",
            "418780700000.0\n",
            "418778100000.0\n",
            "418775400000.0\n",
            "418772700000.0\n",
            "418770030000.0\n",
            "418767370000.0\n",
            "418764620000.0\n",
            "418761930000.0\n",
            "418759300000.0\n",
            "418756530000.0\n",
            "418753840000.0\n",
            "418751150000.0\n",
            "418748470000.0\n",
            "418745840000.0\n",
            "418743120000.0\n",
            "418740440000.0\n",
            "418737800000.0\n",
            "418735060000.0\n",
            "418732380000.0\n",
            "418729700000.0\n",
            "418726970000.0\n",
            "418724350000.0\n",
            "418721660000.0\n",
            "418718970000.0\n",
            "418716300000.0\n",
            "418713600000.0\n",
            "418710900000.0\n",
            "418708230000.0\n",
            "418705570000.0\n",
            "418702900000.0\n",
            "418700200000.0\n",
            "418697500000.0\n",
            "418694820000.0\n",
            "418692140000.0\n",
            "418689420000.0\n",
            "418686760000.0\n",
            "418684040000.0\n",
            "418681320000.0\n",
            "418678640000.0\n",
            "418675950000.0\n",
            "418673260000.0\n",
            "418670640000.0\n",
            "418667900000.0\n",
            "418665200000.0\n",
            "418662550000.0\n",
            "418659860000.0\n",
            "418657100000.0\n",
            "418654500000.0\n",
            "418651800000.0\n",
            "418649080000.0\n",
            "418646330000.0\n",
            "418643640000.0\n",
            "418641000000.0\n",
            "418638300000.0\n",
            "418635640000.0\n",
            "418632930000.0\n",
            "418630300000.0\n",
            "418627620000.0\n",
            "418624960000.0\n",
            "418622240000.0\n",
            "418619520000.0\n",
            "418616770000.0\n",
            "418614050000.0\n",
            "418611400000.0\n",
            "418608740000.0\n",
            "418606100000.0\n",
            "418603400000.0\n",
            "418600750000.0\n",
            "418598060000.0\n",
            "418595300000.0\n",
            "418592600000.0\n",
            "418590000000.0\n",
            "418587280000.0\n",
            "418584600000.0\n",
            "418581940000.0\n",
            "418579220000.0\n",
            "418576500000.0\n",
            "418573800000.0\n",
            "418571120000.0\n",
            "418568440000.0\n",
            "418565750000.0\n",
            "418563060000.0\n",
            "418560440000.0\n",
            "418557660000.0\n",
            "418554940000.0\n",
            "418552350000.0\n",
            "418549560000.0\n",
            "418546970000.0\n",
            "418544300000.0\n",
            "418541570000.0\n",
            "418538900000.0\n",
            "418536200000.0\n",
            "418533500000.0\n",
            "418530900000.0\n",
            "418528200000.0\n",
            "418525480000.0\n",
            "418522730000.0\n",
            "418520040000.0\n",
            "418517350000.0\n",
            "418514700000.0\n",
            "418512000000.0\n",
            "418509230000.0\n",
            "418506570000.0\n",
            "418503950000.0\n",
            "418501260000.0\n",
            "418498580000.0\n",
            "418495920000.0\n",
            "418493170000.0\n",
            "418490550000.0\n",
            "418487760000.0\n",
            "418485140000.0\n",
            "418482500000.0\n",
            "418479770000.0\n",
            "418477100000.0\n",
            "418474400000.0\n",
            "418471770000.0\n",
            "418469000000.0\n",
            "418466300000.0\n",
            "418463680000.0\n",
            "418461020000.0\n",
            "418458240000.0\n",
            "418455550000.0\n",
            "418452830000.0\n",
            "418450200000.0\n",
            "418447560000.0\n",
            "418444840000.0\n",
            "418442150000.0\n",
            "418439460000.0\n",
            "418436740000.0\n",
            "418434100000.0\n",
            "418431430000.0\n",
            "418428750000.0\n",
            "418426060000.0\n",
            "418423370000.0\n",
            "418420700000.0\n",
            "418418000000.0\n",
            "418415300000.0\n",
            "418412600000.0\n",
            "418409970000.0\n",
            "418407200000.0\n",
            "418404500000.0\n",
            "418401800000.0\n",
            "418399130000.0\n",
            "418396540000.0\n",
            "418393750000.0\n",
            "418391130000.0\n",
            "418388400000.0\n",
            "418385630000.0\n",
            "418383040000.0\n",
            "418380350000.0\n",
            "418377660000.0\n",
            "418375040000.0\n",
            "418372350000.0\n",
            "418369630000.0\n",
            "418366950000.0\n",
            "418364260000.0\n",
            "418361570000.0\n",
            "418358850000.0\n",
            "418356100000.0\n",
            "418353480000.0\n",
            "418350830000.0\n",
            "418348070000.0\n",
            "418345420000.0\n",
            "418342800000.0\n",
            "418340040000.0\n",
            "418337420000.0\n",
            "418334740000.0\n",
            "418331950000.0\n",
            "418329260000.0\n",
            "418326600000.0\n",
            "418324020000.0\n",
            "418321330000.0\n",
            "418318580000.0\n",
            "418315930000.0\n",
            "418313240000.0\n",
            "418310550000.0\n",
            "418307800000.0\n",
            "418305150000.0\n",
            "418302460000.0\n",
            "418299770000.0\n",
            "418297120000.0\n",
            "418294400000.0\n",
            "418291800000.0\n",
            "418289100000.0\n",
            "418286400000.0\n",
            "418283700000.0\n",
            "418281000000.0\n",
            "418278300000.0\n",
            "418275620000.0\n",
            "418272940000.0\n",
            "418270200000.0\n",
            "418267530000.0\n",
            "418264840000.0\n",
            "418262220000.0\n",
            "418259440000.0\n",
            "418256750000.0\n",
            "418254060000.0\n",
            "418251370000.0\n",
            "418248750000.0\n",
            "418246070000.0\n",
            "418243440000.0\n",
            "418240700000.0\n",
            "418238000000.0\n",
            "418235320000.0\n",
            "418232600000.0\n",
            "418229900000.0\n",
            "418227220000.0\n",
            "418224540000.0\n",
            "418221820000.0\n",
            "418219200000.0\n",
            "418216600000.0\n",
            "418213860000.0\n",
            "418211230000.0\n",
            "418208450000.0\n",
            "418205760000.0\n",
            "418203070000.0\n",
            "418200420000.0\n",
            "418197730000.0\n",
            "418194980000.0\n",
            "418192360000.0\n",
            "418189670000.0\n",
            "418187000000.0\n",
            "418184300000.0\n",
            "418181640000.0\n",
            "418178960000.0\n",
            "418176300000.0\n",
            "418173580000.0\n",
            "418170900000.0\n",
            "418168200000.0\n",
            "418165520000.0\n",
            "418162840000.0\n",
            "418160180000.0\n",
            "418157500000.0\n",
            "418154740000.0\n",
            "418152020000.0\n",
            "418149400000.0\n",
            "418146650000.0\n",
            "418143960000.0\n",
            "418141270000.0\n",
            "418138650000.0\n",
            "418135930000.0\n",
            "418133250000.0\n",
            "418130560000.0\n",
            "418127870000.0\n",
            "418125200000.0\n",
            "418122500000.0\n",
            "418119800000.0\n",
            "418117200000.0\n",
            "418114500000.0\n",
            "418111800000.0\n",
            "418109130000.0\n",
            "418106400000.0\n",
            "418103720000.0\n",
            "418101040000.0\n",
            "418098350000.0\n",
            "418095700000.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oq2uaVA6Bh1V",
        "colab_type": "code",
        "outputId": "3e5b7561-e0b7-4228-9c2b-a310e9ffa25f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(intercept.numpy(), slope.numpy())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0991884 1.0991763\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAvQGIZACtzW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize an adam optimizer\n",
        "opt = keras.optimizers.Adam(0.5)\n",
        "\n",
        "for j in range(100):\n",
        "\t# Apply minimize, pass the loss function, and supply the variables\n",
        "\topt.minimize(lambda: loss_function(intercept, slope), var_list=[intercept, slope])\n",
        "\n",
        "\t# Print every 10th value of the loss\n",
        "\tif j % 10 == 0:\n",
        "\t\tprint(loss_function(intercept, slope).numpy())\n",
        "\n",
        "# Plot data and regression line\n",
        "plot_results(intercept, slope)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJWA9iImGHKo",
        "colab_type": "text"
      },
      "source": [
        "Note that we've defined a vector of parameters, params, as a variable, rather than using three variables. Here, params[0] is the intercept and params[1] and params[2] are the slopes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd5HJojjGH7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Linear regression using multiple inputs'''\n",
        "\n",
        "# Define the linear regression model\n",
        "def linear_regression(params, feature1 = size_log, feature2 = bedrooms):\n",
        "\treturn params[0] + feature1*params[1] + feature2*params[2]\n",
        "\n",
        "# Define the loss function\n",
        "def loss_function(params, targets = price_log, feature1 = size_log, feature2 = bedrooms):\n",
        "\t# Set the predicted values\n",
        "\tpredictions = linear_regression(params, feature1, feature2)\n",
        "  \n",
        "\t# Use the mean absolute error loss\n",
        "\treturn keras.losses.mae(targets, predictions)\n",
        "\n",
        "# Define the optimize operation\n",
        "opt = keras.optimizers.Adam()\n",
        "\n",
        "# Perform minimization and print trainable variables\n",
        "for j in range(10):\n",
        "\topt.minimize(lambda: loss_function(params), var_list=[params])\n",
        "\tprint_results(params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DxHHfQPGbUl",
        "colab_type": "text"
      },
      "source": [
        "### Batch Training\n",
        "<br>\n",
        "Use for large datasets where os/gpu can't handle chunks over a certain size.  Passes over chunks of data are called epochs.  Pandas and chunksize again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhAlZ0ctHJc-",
        "colab_type": "code",
        "outputId": "031c8eb2-6ab8-4ef6-81d4-0108da2a4c51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define intercept and slope\n",
        "intercept = tf.Variable(0.1, dtype=np.float32)\n",
        "slope = tf.Variable(0.1, dtype=np.float32)\n",
        "\n",
        "# Define a linear regression model\n",
        "def linear_regression(intercept, slope, features):\n",
        "  return intercept + slope*features\n",
        "\n",
        "# Define loss function\n",
        "def loss_function(intercept, slope, targets, features):\n",
        "  predictions = linear_regression(intercept, slope, features)\n",
        "  return tf.keras.losses.mse(targets, predictions)\n",
        "\n",
        "# Define and optimization method\n",
        "opt = tf.keras.optimizers.Adam()\n",
        "\n",
        "# for i in range(1000):\n",
        "#   opt.minimize(lambda: loss_function(intercept, slope), var_list=[intercept, slope])\n",
        "#   if j % 10 == 0:\n",
        "# \t\tprint(loss_function(intercept, slope).numpy())\n",
        "  \n",
        "\n",
        "# Load data in batches\n",
        "for batch in pd.read_csv('kc_house_data.csv', chunksize=100):\n",
        "  # Define targets and predictions\n",
        "  size_batch = np.array(housing[:,5], dtype=np.float32)\n",
        "  price_batch = np.array(housing[:,2], dtype=np.float32) \n",
        "\n",
        "  opt.minimize(lambda: loss_function(intercept, slope, price_batch, size_batch), var_list=[intercept, slope])\n",
        "\n",
        "  print(intercept.numpy(), slope.numpy())  \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.101 0.101\n",
            "0.102 0.102\n",
            "0.103 0.103\n",
            "0.103999995 0.103999995\n",
            "0.10499999 0.10499999\n",
            "0.10599999 0.10599999\n",
            "0.106999986 0.106999986\n",
            "0.10799998 0.10799998\n",
            "0.108999975 0.108999975\n",
            "0.10999997 0.10999997\n",
            "0.110999964 0.110999964\n",
            "0.11199996 0.11199996\n",
            "0.11299995 0.11299995\n",
            "0.11399995 0.11399995\n",
            "0.11499994 0.11499994\n",
            "0.11599994 0.11599994\n",
            "0.11699993 0.11699993\n",
            "0.11799992 0.11799992\n",
            "0.118999906 0.118999906\n",
            "0.11999989 0.11999989\n",
            "0.12099988 0.12099988\n",
            "0.12199987 0.12199987\n",
            "0.122999854 0.122999854\n",
            "0.123999834 0.123999834\n",
            "0.124999814 0.124999814\n",
            "0.1259998 0.1259998\n",
            "0.12699977 0.12699977\n",
            "0.12799974 0.12799974\n",
            "0.12899971 0.12899971\n",
            "0.12999968 0.12999968\n",
            "0.13099965 0.13099965\n",
            "0.13199963 0.13199963\n",
            "0.1329996 0.1329996\n",
            "0.13399957 0.13399957\n",
            "0.13499954 0.13499954\n",
            "0.1359995 0.1359995\n",
            "0.13699946 0.13699946\n",
            "0.13799942 0.13799942\n",
            "0.13899937 0.13899937\n",
            "0.13999933 0.13999933\n",
            "0.14099929 0.14099929\n",
            "0.14199924 0.14199924\n",
            "0.1429992 0.1429992\n",
            "0.14399916 0.14399916\n",
            "0.1449991 0.14499912\n",
            "0.14599904 0.14599906\n",
            "0.14699899 0.146999\n",
            "0.14799893 0.14799894\n",
            "0.14899887 0.14899889\n",
            "0.14999881 0.14999883\n",
            "0.15099876 0.15099877\n",
            "0.1519987 0.15199871\n",
            "0.15299863 0.15299866\n",
            "0.15399855 0.15399858\n",
            "0.15499848 0.15499851\n",
            "0.15599841 0.15599844\n",
            "0.15699834 0.15699837\n",
            "0.15799826 0.1579983\n",
            "0.15899819 0.15899822\n",
            "0.15999812 0.15999815\n",
            "0.16099805 0.16099808\n",
            "0.16199796 0.16199799\n",
            "0.16299787 0.1629979\n",
            "0.16399778 0.16399781\n",
            "0.1649977 0.16499773\n",
            "0.16599761 0.16599764\n",
            "0.16699752 0.16699755\n",
            "0.16799743 0.16799746\n",
            "0.16899733 0.16899738\n",
            "0.16999723 0.16999727\n",
            "0.17099713 0.17099717\n",
            "0.17199703 0.17199707\n",
            "0.17299692 0.17299697\n",
            "0.17399682 0.17399687\n",
            "0.17499672 0.17499676\n",
            "0.17599662 0.17599666\n",
            "0.1769965 0.17699656\n",
            "0.17799638 0.17799644\n",
            "0.17899626 0.17899632\n",
            "0.17999615 0.1799962\n",
            "0.18099603 0.18099609\n",
            "0.18199591 0.18199597\n",
            "0.1829958 0.18299586\n",
            "0.18399568 0.18399574\n",
            "0.18499555 0.18499562\n",
            "0.18599541 0.18599549\n",
            "0.18699528 0.18699536\n",
            "0.18799515 0.18799523\n",
            "0.18899502 0.1889951\n",
            "0.18999489 0.18999496\n",
            "0.19099475 0.19099483\n",
            "0.19199462 0.1919947\n",
            "0.19299448 0.19299456\n",
            "0.19399433 0.19399442\n",
            "0.19499418 0.19499427\n",
            "0.19599403 0.19599412\n",
            "0.19699389 0.19699398\n",
            "0.19799374 0.19799383\n",
            "0.1989936 0.19899368\n",
            "0.19999345 0.19999354\n",
            "0.2009933 0.20099339\n",
            "0.20199314 0.20199323\n",
            "0.20299298 0.20299307\n",
            "0.20399281 0.2039929\n",
            "0.20499265 0.20499274\n",
            "0.20599249 0.20599258\n",
            "0.20699233 0.20699242\n",
            "0.20799217 0.20799226\n",
            "0.20899199 0.2089921\n",
            "0.20999181 0.20999193\n",
            "0.21099164 0.21099176\n",
            "0.21199146 0.21199158\n",
            "0.21299128 0.2129914\n",
            "0.2139911 0.21399122\n",
            "0.21499093 0.21499105\n",
            "0.21599075 0.21599087\n",
            "0.21699056 0.2169907\n",
            "0.21799037 0.21799052\n",
            "0.21899018 0.21899033\n",
            "0.21998999 0.21999013\n",
            "0.2209898 0.22098994\n",
            "0.2219896 0.22198975\n",
            "0.22298941 0.22298956\n",
            "0.22398922 0.22398937\n",
            "0.22498901 0.22498918\n",
            "0.2259888 0.22598898\n",
            "0.2269886 0.22698878\n",
            "0.22798839 0.22798857\n",
            "0.22898819 0.22898836\n",
            "0.22998798 0.22998816\n",
            "0.23098777 0.23098795\n",
            "0.23198757 0.23198774\n",
            "0.23298736 0.23298754\n",
            "0.23398714 0.23398733\n",
            "0.23498692 0.23498711\n",
            "0.2359867 0.23598689\n",
            "0.23698647 0.23698667\n",
            "0.23798625 0.23798645\n",
            "0.23898603 0.23898622\n",
            "0.23998581 0.239986\n",
            "0.24098559 0.24098578\n",
            "0.24198535 0.24198556\n",
            "0.24298511 0.24298532\n",
            "0.24398488 0.24398509\n",
            "0.24498464 0.24498485\n",
            "0.2459844 0.24598461\n",
            "0.24698417 0.24698438\n",
            "0.24798393 0.24798414\n",
            "0.2489837 0.2489839\n",
            "0.24998344 0.24998367\n",
            "0.2509832 0.25098342\n",
            "0.25198296 0.25198317\n",
            "0.2529827 0.2529829\n",
            "0.25398245 0.25398266\n",
            "0.2549822 0.2549824\n",
            "0.25598195 0.25598216\n",
            "0.2569817 0.2569819\n",
            "0.25798145 0.25798166\n",
            "0.2589812 0.2589814\n",
            "0.25998095 0.25998116\n",
            "0.2609807 0.2609809\n",
            "0.2619804 0.26198065\n",
            "0.26298013 0.2629804\n",
            "0.26397985 0.26398012\n",
            "0.26497957 0.26497984\n",
            "0.2659793 0.26597956\n",
            "0.266979 0.26697928\n",
            "0.26797873 0.267979\n",
            "0.26897845 0.26897871\n",
            "0.26997817 0.26997843\n",
            "0.27097788 0.27097815\n",
            "0.2719776 0.27197787\n",
            "0.27297732 0.2729776\n",
            "0.27397704 0.2739773\n",
            "0.27497676 0.27497703\n",
            "0.27597648 0.27597675\n",
            "0.2769762 0.27697647\n",
            "0.2779759 0.27797619\n",
            "0.27897558 0.2789759\n",
            "0.27997527 0.27997562\n",
            "0.28097495 0.2809753\n",
            "0.28197464 0.281975\n",
            "0.28297433 0.2829747\n",
            "0.28397402 0.28397438\n",
            "0.2849737 0.28497407\n",
            "0.2859734 0.28597376\n",
            "0.2869731 0.28697345\n",
            "0.28797278 0.28797314\n",
            "0.28897247 0.28897282\n",
            "0.28997216 0.2899725\n",
            "0.29097185 0.2909722\n",
            "0.29197153 0.2919719\n",
            "0.29297122 0.29297158\n",
            "0.2939709 0.29397127\n",
            "0.29497057 0.29497096\n",
            "0.29597023 0.29597065\n",
            "0.2969699 0.2969703\n",
            "0.29796955 0.29796997\n",
            "0.2989692 0.29896963\n",
            "0.29996887 0.2999693\n",
            "0.30096853 0.30096895\n",
            "0.3019682 0.3019686\n",
            "0.30296785 0.30296826\n",
            "0.3039675 0.30396792\n",
            "0.30496716 0.30496758\n",
            "0.30596682 0.30596724\n",
            "0.30696648 0.3069669\n",
            "0.30796614 0.30796656\n",
            "0.3089658 0.30896622\n",
            "0.30996546 0.30996588\n",
            "0.31096512 0.31096554\n",
            "0.31196475 0.3119652\n",
            "0.31296438 0.31296486\n",
            "0.313964 0.3139645\n",
            "0.31496364 0.31496412\n",
            "0.31596327 0.31596375\n",
            "0.3169629 0.31696337\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vs_vWVODgtom",
        "colab_type": "text"
      },
      "source": [
        "# Neural Layers\n",
        "\n",
        "### Dense Layers\n",
        "input layer (features) --> hidden dense layer(s) sum(features*weights)into activation function --> output layer (prediction)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1UmSr1KKw_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Method that uses matrix multiplication (linear algebra)'''\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define features(inputs)\n",
        "features = tf.constant([1,35])\n",
        "\n",
        "# Define weights\n",
        "weights = tf.Variable([-.05],[-0.1])\n",
        "\n",
        "# Define bias\n",
        "bias = tf.Variable([0.5])\n",
        "\n",
        "# Multiply inputs by weights\n",
        "product = tf.matmul(features, weights)\n",
        "\n",
        "# Define dense layer with activation function\n",
        "dense = tf.keras.activations.sigmoid(product+bias)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vi_yXceKtVoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Method that uses higher-level/complete inbuilts'''\n",
        "\n",
        "# Define input layer\n",
        "features = tf.constant(data, tf.float32)\n",
        "\n",
        "# Define first dense layer\n",
        "dense = tf.keras.layers.Dense(10, activation='sigmoid')(inputs) # bias included but not defined by user\n",
        "\n",
        "# Define second and third layers\n",
        "dense2 = tf.keras.layers.Dense(5,activation='sigmoid')(dense)\n",
        "output = tf.keras.layers.Dense(1,activation='sigmoid')(dense2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HcGOjK6v1E3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize bias1\n",
        "bias1 = Variable(1.0)\n",
        "\n",
        "# Initialize weights1 as 3x2 variable of ones\n",
        "weights1 = Variable(ones((3, 2))) # why are there two sets of parentheses?\n",
        "\n",
        "# Perform matrix multiplication of borrower_features and weights1\n",
        "product1 = matmul(borrower_features,weights1)\n",
        "\n",
        "# Apply sigmoid activation function to product1 + bias1\n",
        "dense1 = keras.activations.Dense(product1 + bias)\n",
        "\n",
        "# Print shape of dense1\n",
        "print(\"\\n dense1's output shape: {}\".format(dense1.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSiG5k6ZxjfN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From previous step\n",
        "bias1 = Variable(1.0)\n",
        "weights1 = Variable(ones((3, 2)))\n",
        "product1 = matmul(borrower_features, weights1)\n",
        "dense1 = keras.activations.sigmoid(product1 + bias1)\n",
        "\n",
        "# Initialize bias2 and weights2\n",
        "bias2 = Variable(1.0)\n",
        "weights2 = Variable(ones((2,1)))\n",
        "\n",
        "# Perform matrix multiplication of dense1 and weights2\n",
        "product2 = matmul(dense1,weights2)\n",
        "\n",
        "# Apply activation to product2 + bias2 and print the prediction\n",
        "prediction = keras.activations.sigmoid(product2+bias2)\n",
        "print('\\n prediction: {}'.format(prediction.numpy()[0,0]))\n",
        "print('\\n actual: 1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4TalXzlyG62",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute the product of borrower_features and weights1\n",
        "products1 = matmul(borrower_features,weights1)\n",
        "\n",
        "# Apply a sigmoid activation function to products1 + bias1\n",
        "dense1 = keras.activations.sigmoid(products1+bias1)\n",
        "\n",
        "# Print the shapes of borrower_features, weights1, bias1, and dense1\n",
        "print('\\n shape of borrower_features: ', borrower_features.shape)\n",
        "print('\\n shape of weights1: ', weights1.shape)\n",
        "print('\\n shape of bias1: ', bias1.shape)\n",
        "print('\\n shape of dense1: ', dense1.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lqx4dhiMys-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Define the first dense layer\n",
        "dense1 = keras.layers.Dense(7, activation='sigmoid')(borrower_features)\n",
        "\n",
        "# Define a dense layer with 3 output nodes\n",
        "dense2 = keras.layers.Dense(3,activation='sigmoid')(dense1)\n",
        "\n",
        "# Define a dense layer with 1 output node\n",
        "predictions = keras.layers.Dense(1,activation='sigmoid')(dense2)\n",
        "\n",
        "# Print the shapes of dense1, dense2, and predictions\n",
        "print('\\n shape of dense1: ', dense1.shape)\n",
        "print('\\n shape of dense2: ', dense2.shape)\n",
        "print('\\n shape of predictions: ', predictions.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqPgUCgsyxpQ",
        "colab_type": "text"
      },
      "source": [
        "### Activation functions\n",
        "<br>\n",
        "linear matrix multiplication (inputs x weights), non linear activation (shaping) function for first dense layer (eg sigmoid)\n",
        "<br>\n",
        "1. Sigmoid - binary classification function, hidden layers<br>\n",
        "2. ReLu -  hidden layers, accepts max of value passed to it and 0<br>\n",
        "3. Softmax - used in output layer in classification problems in multiclass classification problems.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcGKBWKn31M8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define inputs\n",
        "inputs = tf.constant(borrower_features, tf.float32)\n",
        "\n",
        "# Define dense layer 1\n",
        "dense = tf.keras.layers.Dense(16, activation='relu')(borrower_features)\n",
        "\n",
        "# Define dense layer 2\n",
        "dense2 = tf.keras.layers.Dense(8, activation='sigmoid')(dense) # note switch to sigmoid\n",
        "\n",
        "# Define output\n",
        "output = tf.keras.layers.Dense(4, activation='softmax')(dense2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ixr3vLX5u4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Construct input layer from features\n",
        "inputs = constant(bill_amounts, float32)\n",
        "\n",
        "# Define first dense layer\n",
        "dense1 = keras.layers.Dense(3, activation='relu')(inputs)\n",
        "\n",
        "# Define second dense layer\n",
        "dense2 = keras.layers.Dense(2, activation='relu')(dense1)\n",
        "\n",
        "# Define output layer\n",
        "outputs = keras.layers.Dense(1, activation='sigmoid')(dense1)\n",
        "\n",
        "# Print error for first five examples\n",
        "error = default[:5] - outputs.numpy()[:5]\n",
        "print(error)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5xSnu2j6zpi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Construct input layer from borrower features\n",
        "inputs = constant(borrower_features, float32)\n",
        "\n",
        "# Define first dense layer\n",
        "dense1 = keras.layers.Dense(10, activation='sigmoid')(inputs)\n",
        "\n",
        "# Define second dense layer\n",
        "dense2 = keras.layers.Dense(8, activation='relu')(dense1)\n",
        "\n",
        "# Define output layer\n",
        "outputs = keras.layers.Dense(6, activation='softmax')(dense2)\n",
        "\n",
        "# Print first five predictions\n",
        "print(outputs.numpy()[:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRI6oDvg62gX",
        "colab_type": "text"
      },
      "source": [
        "### Optimizers\n",
        "<br>\n",
        "Gradient Descent <br>\n",
        "Stochastic gradient descent is less likely to get stuck in local minimums.<br>\n",
        "Pass in a learning rate between 0.5 and 0.001.  Higher learning rates can cause function overshoot global minimum. <br>\n",
        "<br>\n",
        "Root Mean Squared (RMS) optimizer<br>\n",
        "Applies different learning rates to each feature.  <br>\n",
        "Pass in learning rate, momentum and decay.  Setting the decay to a low value prevents momentum from building too much. <br>\n",
        "<br>\n",
        "Adaptive Moment (Adam) optimizer<br>\n",
        "Generally a good first choice because it performs better with default parameter values.\n",
        "Pass in learning rate and beta1. Lower beta1 to have momentum decay faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTlDy5Oc64Bc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define model function\n",
        "def model(bias, weights, features = borrower_features):\n",
        "  product = tf.matmul(features, weights)\n",
        "  return tf.keras.activations.sigmoid(product+bias)\n",
        "\n",
        "# Define loss function, computes predicted values and loss\n",
        "def loss_function(bias, weights, targets=default, features=borrower_features):\n",
        "  predictions = model(bias, weights)\n",
        "  return tf.keras.losses.binary_crossentropy(targets, predictions)  \n",
        "\n",
        "# Minimize loss function\n",
        "opt = tf.keras.optimizers.RMSprop(learning_rate=0.01,momentum=0.9)\n",
        "opt.minimize(lambda: loss_function(bias, weights), var_list=[bias,weights])  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxEyMdrUBtxF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize x_1 and x_2\n",
        "x_1 = Variable(6.0,float32)\n",
        "x_2 = Variable(0.3,float32)\n",
        "\n",
        "# Define the optimization operation\n",
        "opt = keras.optimizers.SGD(learning_rate=0.01)\n",
        "\n",
        "for j in range(100):\n",
        "\t# Perform minimization using the loss function and x_1\n",
        "\topt.minimize(lambda: loss_function(x_1), var_list=[x_1])\n",
        "\t# Perform minimization using the loss function and x_2\n",
        "\topt.minimize(lambda: loss_function(x_2), var_list=[x_2])\n",
        "\n",
        "# Print x_1 and x_2 as numpy arrays\n",
        "print(x_1.numpy(), x_2.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLhH5kCjCcSd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Initialize x_1 and x_2\n",
        "x_1 = Variable(0.05,float32)\n",
        "x_2 = Variable(0.05,float32)\n",
        "\n",
        "# Define the optimization operation for opt_1 and opt_2\n",
        "opt_1 = keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.99)\n",
        "opt_2 = keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.00)\n",
        "\n",
        "for j in range(100):\n",
        "\topt_1.minimize(lambda: loss_function(x_1), var_list=[x_1])\n",
        "    # Define the minimization operation for opt_2\n",
        "\topt_2.minimize(lambda: loss_function(x_2), var_list=[x_2])\n",
        "\n",
        "# Print x_1 and x_2 as numpy arrays\n",
        "print(x_1.numpy(), x_2.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-p_RXm6ChBW",
        "colab_type": "text"
      },
      "source": [
        "### Training a Network in Tensorflow\n",
        "<br>\n",
        "Use random values taken from normal distributions to initialize varibles in large datasets: ones and doing it by hand wont work. Can also use Glorot initializers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ul771m1bFr3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Choosing random weights\n",
        "\n",
        "# Define a 500x500 normal variable\n",
        "weights = tf.Variable(tf.random.normal([500,500]))\n",
        "\n",
        "# Define a truncated 500x500 random normal variable\n",
        "weights = tf.Variable(tf.random.truncated_normal([500,500])) # gets rid of large and small numbers in set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMzRm-NbGmfY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define dense layer with default initializer\n",
        "dense = tf.keras.layers.Dense(32, activation='relu')\n",
        "\n",
        "# Define dense layer with zeros initializer\n",
        "dense = tf.keras.layers.Dense(32, activation='relu', kernel_initializer='zeros')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQc1A3OCISCO",
        "colab_type": "text"
      },
      "source": [
        "#### Neural Networks and Overfitting: Apply dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHfwypnsIURf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Define input data, dense layers\n",
        "inputs = np.array(borrower_features, float32)\n",
        "dense = tf.keras.layers.Dense(32, activation='relu')(inputs)\n",
        "dense2 = tf.keras.layers.Dense(16, activation='relu')(dense)\n",
        "\n",
        "# Apply dropout operation\n",
        "dropout = tf.keras.layers.dropout(0.25)(dense2)\n",
        "\n",
        "# Create outputs\n",
        "output = tf.keras.layers.Dense(1, activation='sigmoid')(dropout)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzZjzmG7KiJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the layer 1 weights\n",
        "w1 = Variable(random.normal([23, 7]))\n",
        "\n",
        "# Initialize the layer 1 bias\n",
        "b1 = Variable(ones([7]))\n",
        "\n",
        "# Define the layer 2 weights\n",
        "w2 = Variable(random.normal([7,1]))\n",
        "\n",
        "# Define the layer 2 bias\n",
        "b2 = Variable([0.0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJGXsyvFLqRG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Define the model\n",
        "def model(w1, b1, w2, b2, features = borrower_features):\n",
        "\t# Apply relu activation functions to layer 1\n",
        "\tlayer1 = keras.activations.relu(matmul(features, w1) + b1)\n",
        "    # Apply dropout\n",
        "\tdropout = keras.layers.Dropout(0.25)(layer1)\n",
        "\treturn keras.activations.sigmoid(matmul(dropout, w2) + b2)\n",
        "\n",
        "# Define the loss function\n",
        "def loss_function(w1, b1, w2, b2, features = borrower_features, targets = default):\n",
        "\tpredictions = model(w1, b1, w2, b2)\n",
        "\t# Pass targets and predictions to the cross entropy loss\n",
        "\treturn keras.losses.binary_crossentropy(targets, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MipIWPrLqgt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Train the model\n",
        "for j in range(100):\n",
        "    # Complete the optimizer\n",
        "\topt.minimize(lambda: loss_function(w1, b1, w2, b2), \n",
        "                 var_list=[w1, b1, w2, b2])\n",
        "\n",
        "# Make predictions with model\n",
        "model_predictions = model(w1, b1, w2, b2, test_features)\n",
        "\n",
        "# Construct the confusion matrix\n",
        "confusion_matrix(test_targets, model_predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21FKqgVEp8om",
        "colab_type": "text"
      },
      "source": [
        "# High Level API's \n",
        "\n",
        "### Defining Neural Networks with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXrBmQhzqsGl",
        "colab_type": "text"
      },
      "source": [
        "The sequential API:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unlDLKiLqH-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Define model\n",
        "model = keras.Sequential()\n",
        "\n",
        "# Define first hidden layer\n",
        "model.add(keras.layers.Dense(16, activation='relu', inputshape=(28*28,)))\n",
        "\n",
        "# Define second hidden layer\n",
        "model.add(keras.layers.Dense(8, activation='relu'))\n",
        "\n",
        "# Define output layer\n",
        "model.add(keras.layers.Dense(4, activation='softmax'))\n",
        "\n",
        "#model.summary() # returns summary of model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OxNC5aFst32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile the model\n",
        "model.compile('adam', loss='categorical_crossentropy')\n",
        "\n",
        "# Summarize the model\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkG2Z9mPvg0w",
        "colab_type": "text"
      },
      "source": [
        "Using the functional api for training two models at once:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3UPKLpwvki9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define model 1 input layer shape\n",
        "model1_inputs = tf.keras.Input(shape=(28*28,))\n",
        "\n",
        "# Define model 2 input layer shape\n",
        "model2_inputs = tf.keras.Input(shape=(10,))\n",
        "\n",
        "\n",
        "\n",
        "# Define layer 1 for model 1\n",
        "model1_layer1 = tf.keras.layers.Dense(12, activation='relu')(model1_inputs)\n",
        "\n",
        "# Define layer 2 for model 1\n",
        "model1_layer2 = tf.keras.layers.Dense(4, activation='softmax')(model1_layer1)\n",
        "\n",
        "\n",
        "# Define layer 1 for model 2\n",
        "model2_layer1 = tf.keras.layers.Dense(8, activation='relu')(model2_inputs)\n",
        "\n",
        "# Define layer 2 for model 1\n",
        "model2_layer2 = tf.keras.layers.Dense(4, activation='softmax')(model2_layer1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BqVW_UyxiMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Merge model 1 and model 2\n",
        "merged = tf.keras.layers.add([model1_layer2, model2_layer2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8WmYfy7x0Gw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a functional model\n",
        "model = tf.keras.Model(inputs=[model1_layer2, model2_layer2], outputs=merged)\n",
        "\n",
        "# Compile the model\n",
        "model.compile('adam', loss='categorical_crossentropy')\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYuuogiTy8JL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# given\n",
        "\n",
        "# Define a Keras sequential model\n",
        "model = keras.Sequential()\n",
        "\n",
        "# Define the first dense layer\n",
        "model.add(keras.layers.Dense(16, activation='relu', input_shape=(784,)))\n",
        "\n",
        "# Define the second dense layer\n",
        "model.add(keras.layers.Dense(8, activation='relu'))\n",
        "\n",
        "# Define the output layer\n",
        "model.add(keras.layers.Dense(4, activation='softmax'))\n",
        "\n",
        "# Print the model architecture\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DH3Yr28czwZC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# given\n",
        "\n",
        "# Define the first dense layer\n",
        "model.add(keras.layers.Dense(16, activation='sigmoid', input_shape=(784,)))\n",
        "\n",
        "# Apply dropout to the first layer's output\n",
        "model.add(keras.layers.Dropout(0.25))\n",
        "\n",
        "# Define the output layer\n",
        "model.add(keras.layers.Dense(4, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile('adam', loss='categorical_crossentropy')\n",
        "\n",
        "# Print a model summary\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ov2x0ah0YTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# For model 1, pass the input layer to layer 1 and layer 1 to layer 2\n",
        "m1_layer1 = keras.layers.Dense(12, activation='sigmoid')(m1_inputs)\n",
        "m1_layer2 = keras.layers.Dense(4, activation='softmax')(m1_layer1)\n",
        "\n",
        "# For model 2, pass the input layer to layer 1 and layer 1 to layer 2\n",
        "m2_layer1 = keras.layers.Dense(12, activation='relu')(m2_inputs)\n",
        "m2_layer2 = keras.layers.Dense(4, activation='softmax')(m2_layer1)\n",
        "\n",
        "# Merge model outputs and define a functional model\n",
        "merged = keras.layers.add([m1_layer2, m2_layer2])\n",
        "model = keras.Model(inputs=[m1_inputs, m2_inputs], outputs=merged)\n",
        "\n",
        "# Print a model summary\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_Ous47XrxPD",
        "colab_type": "text"
      },
      "source": [
        "### Training and Validation with Keras\n",
        "<br>\n",
        "Steps: load and clean data, define model, train and validate model, evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iODAxfkr3-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Define a sequential model\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "# Define hidden layer\n",
        "model.add(tf.keras.layers.Dense(16, activation='relu', input_shape=(784,)))\n",
        "\n",
        "# Define output layer\n",
        "model.add(tf.keras.Dense(4, activation='softmax'))\n",
        "\n",
        "# Compile\n",
        "model.compile('adam', loss='categorical_crossentropy')\n",
        "\n",
        "# Train model\n",
        "model.fit(image_features, image_labels) # additional parameters include batchsize(32 by default) and epoch (# of times trained on the full set of batches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zQjgQmcuIV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training model with validation split, creates section of training set for validating\n",
        "model.fit(features, labels, epochs=10, validation_split=0.20) # if training loss falls much below validation loss, \n",
        "# this indicates that the model is overfitting.  Either shorten training or add regularization(?) or dropout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vvkpUwZvPQM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add metrics= parameter to model.compile get percentage of accurately classified data points insted of \n",
        "model.compile('adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# Same .fit parameters\n",
        "model.fit(features, labels, epochs=10, validation_split=0.20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hs8rVXGwgH0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate the test set\n",
        "model.evaluate(test)  # When did we separate out the test set?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39t_EuiKxtee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Given\n",
        "\n",
        "# Define a sequential model\n",
        "model = keras.Sequential()\n",
        "\n",
        "# Define a hidden layer\n",
        "model.add(keras.layers.Dense(16, activation='relu', input_shape=(784,)))\n",
        "\n",
        "# Define the output layer\n",
        "model.add(keras.layers.Dense(4, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile('SGD', loss='categorical_crossentropy')\n",
        "\n",
        "# Complete the fitting operation\n",
        "model.fit(sign_language_features, sign_language_labels, epochs=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQCJoVDExu4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define sequential model\n",
        "model = keras.Sequential()\n",
        "\n",
        "# Define the first layer\n",
        "model.add(keras.layers.Dense(32, activation='sigmoid', input_shape=(784,)))\n",
        "\n",
        "# Add activation function to classifier\n",
        "model.add(keras.layers.Dense(4, activation='softmax'))\n",
        "\n",
        "# Set the optimizer, loss function, and metrics\n",
        "model.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Add the number of epochs and the validation split\n",
        "model.fit(sign_language_features, sign_language_labels, epochs=10, validation_split=0.10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7b6xMBazqS7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define sequential model\n",
        "model = keras.Sequential()\n",
        "\n",
        "# Define the first layer\n",
        "model.add(keras.layers.Dense(1024, activation='relu', input_shape=(784,)))\n",
        "\n",
        "# Add activation function to classifier\n",
        "model.add(keras.layers.Dense(4, activation='softmax'))\n",
        "\n",
        "# Finish the model compilation\n",
        "model.compile(optimizer=keras.optimizers.Adam(lr=0.01), \n",
        "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Complete the model fit operation\n",
        "model.fit(sign_language_features, sign_language_labels, epochs=200, validation_split=0.50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5370FWVO0x1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate the small model using the train data\n",
        "small_train = small_model.evaluate(train_features, train_labels)\n",
        "\n",
        "# Evaluate the small model using the test data\n",
        "small_test = small_model.evaluate(test_features, test_labels)\n",
        "\n",
        "# Evaluate the large model using the train data\n",
        "large_train = large_model.evaluate(train_features, train_labels)\n",
        "\n",
        "# Evaluate the large model using the test data\n",
        "large_test = large_model.evaluate(test_features, test_labels)\n",
        "\n",
        "# Print losses\n",
        "print('\\n Small - Train: {}, Test: {}'.format(small_train, small_test))\n",
        "print('Large - Train: {}, Test: {}'.format(large_train, large_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xN6uKuB602L9",
        "colab_type": "text"
      },
      "source": [
        "### Training Models with Estimators API\n",
        "<br>\n",
        "Order of Operations: define feature columns, load and transform data, define an estimator, apply train operation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylAC8fZ01C8Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining a numeric feature column from kc housing set\n",
        "size = tf.feature_colum.numeric_column('size')\n",
        "\n",
        "# Define a categorical feature column\n",
        "rooms = tf.feature_colum.categorical_column_with_vocabulary_list('rooms',['1','2','3','4','5'])\n",
        "\n",
        "# Create feature list\n",
        "features = [size, rooms]\n",
        "\n",
        "# Define a matrix feature column (if using images)\n",
        "features = [tf.feature_colum.numeric_column('image',shape=(784,))]\n",
        "\n",
        "# Define input function\n",
        "def input_fxn():\n",
        "  # Define feature dictionary\n",
        "  features = {'size':[1340, 1620, 1790],'rooms':[1,3,4]}\n",
        "  # Define labels\n",
        "  labels = [229000,538000,180000]\n",
        "  return features, labels\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPkvjXSj5Vj7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a deep neural network REGRESSION\n",
        "model = tf.estimator.DNNRegressor(feature_colomns=features, hidden_units=[10,6,6,3]) # hidden units is number of nodes in each hidden layer\n",
        "\n",
        "# Train the regression model\n",
        "model.train(input_fxn, steps=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZGUXfWI5XHF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a deep neural network CLASSIFICATION\n",
        "model = tf.estimator.DNNClassifier(feature_columns=features, hidden_units=[32,16,8], n_classes=4)\n",
        "\n",
        "# Train the classifier\n",
        "model.train(input_fxn, steps=20)\n",
        "\n",
        "#https://www.tensorflow.org/guides/estimators"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD2hWUfV6mh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''How to pass in datafram colums to the input function'''\n",
        "\n",
        "# Define feature columns for bedrooms and bathrooms\n",
        "bedrooms = feature_column.numeric_column(\"bedrooms\")\n",
        "bathrooms = feature_column.numeric_column(\"bathrooms\")\n",
        "\n",
        "# Define the list of feature columns\n",
        "feature_list = [bedrooms, bathrooms]\n",
        "\n",
        "def input_fn():\n",
        "\t# Define the labels\n",
        "\tlabels = np.array(housing['price'])\n",
        "\t# Define the features\n",
        "\tfeatures = {'bedrooms':np.array(housing['bedrooms']), \n",
        "                'bathrooms':np.array(housing['bathrooms'])}\n",
        "\treturn features, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l52t_MzW7H09",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the model and set the number of steps\n",
        "model = estimator.DNNRegressor(feature_columns=feature_list, hidden_units=[2,2])\n",
        "model.train(input_fn, steps=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-sJFOL07Ieg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the model and set the number of steps\n",
        "model = estimator.LinearRegressor(feature_columns=feature_list)\n",
        "model.train(input_fn, steps=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsxJniW17yE7",
        "colab_type": "text"
      },
      "source": [
        "Checkout tensorflowhub (good for testing images on a model that has already been trained on a huge dataset) <br>\n",
        "Tensorflow Probability (random number generators, optimizers for statistical models)"
      ]
    }
  ]
}